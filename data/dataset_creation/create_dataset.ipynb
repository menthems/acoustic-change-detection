{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270c7e11",
   "metadata": {},
   "source": [
    "# Dataset Creation - TRsim\n",
    "\n",
    "## Step 1: Create the foundation files\n",
    "\n",
    "Within the folder data/scene_datasets/TRsim/stages/:\n",
    "\n",
    "- TRsim_room.glb: floor + walls + ceiling mesh.\n",
    "\n",
    "- TRsim_semantic.ply: export the room, as well as the box.\n",
    "\n",
    "- TRsim.house: .house file describing the room geometry, and the box location.\n",
    "\n",
    "Within the folder data/scene_datasets/TRsim/objects/:\n",
    "\n",
    "- TRsim_box.glb: box mesh.\n",
    "\n",
    "## Step 1a: Readout human locations\n",
    "\n",
    "Read out human locations from the Human1 SoundCAM dataset, save the locxyz per N (to later be able to have the location based on the filename).\n",
    "\n",
    "## Step 2: Modify the _semantic.ply\n",
    "\n",
    "Modify the .ply file to change the last 8 vertices to different locations, as well as adding the right material \"object_id\". Save in the format \"N_semantic.ply\".\n",
    "\n",
    "## Step 3: Run SoundSpaces 2.0\n",
    "\n",
    "Load the file, rename to \"treated_room_object_semantic.ply\", run simulation, and save as \"N.wav\".\n",
    "\n",
    "## Step 4: Create spectrograms\n",
    "\n",
    "Load the .wav files, and create spectrograms, which are saved as \"N.npy\" (or other fileformat? Or as part of the model input preprocessing?).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
